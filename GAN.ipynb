{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage.io import imsave\n",
    "from tensorflow.examples.tutorials.mnist import input_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 将下载好的MNIST_data数据解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义DataLoad()函数将文件数据转为numpy可以读取的格式\n",
    "def DataLoad(data_path):\n",
    "    file_data = open(os.path.join(data_path,'train-images.idx3-ubyte'))\n",
    "    loaded_data = np.fromfile(file = file_data,dtype = np.uint8)\n",
    "    #前16个字符为说明符，需要跳过\n",
    "    train_data = loaded_data[16:].reshape((-1,784)).astype(np.float)\n",
    "    \n",
    "    file_label = open(os.path.join(data_path,'train-labels.idx1-ubyte'))\n",
    "    loaded_label = np.fromfile(file = file_label,dtype = np.uint8)\n",
    "    #前8个字符为说明符，需要跳过\n",
    "    train_label = loaded_label[8:].reshape((-1)).astype(np.float)\n",
    "    \n",
    "    return train_data,train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图像的size为（28,28,1）\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "image_size = image_width * image_height\n",
    "\n",
    "#是否训练和存储设置\n",
    "train = True\n",
    "restore = False\n",
    "output_path = \"./output_image/\"\n",
    "\n",
    "# set hyperparameters\n",
    "max_epoch = 300\n",
    "batch_size = 256\n",
    "z_size = 220            #生成器的传入参数\n",
    "h1_size = 300     #第一隐藏层的size，即特征数\n",
    "h2_size = 300     #第二隐藏层的size，即特征数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 构建生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def Generator(z_input):\n",
    "    #第一个链接层\n",
    "    w1 = tf.Variable(tf.truncated_normal([z_size,h1_size],stddev = 0.1),name = \"g_w1\",dtype = tf.float32)\n",
    "    b1 = tf.Variable(tf.zeros([h1_size]),name = \"g_b1\",dtype = tf.float32)\n",
    "    h1 = tf.nn.relu(tf.matmul(z_input, w1) + b1)\n",
    "    \n",
    "    #第二个链接层\n",
    "    w2 = tf.Variable(tf.truncated_normal([h1_size,h2_size],stddev = 0.1),name = \"g_w2\",dtype = tf.float32)\n",
    "    b2 = tf.Variable(tf.zeros([h2_size]),name = \"g_b2\",dtype = tf.float32)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    \n",
    "    #第三个链接层\n",
    "    w3 = tf.Variable(tf.truncated_normal([h2_size,image_size],stddev = 0.1),name = \"g_w3\",dtype = tf.float32)\n",
    "    b3 = tf.Variable(tf.zeros([image_size]),name = \"g_b3\",dtype = tf.float32)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, w3) + b3)\n",
    "    \n",
    "    #输出：生成图像\n",
    "    output_generate = tf.nn.tanh(h3)   #利用tanh激活函数，将h3传入输出层\n",
    "    \n",
    "    #输出：生成图像的所有参数\n",
    "    g_parameters = [w1, w2, w3, b1, b2, b3]               #合并所有参数\n",
    "    \n",
    "    return output_generate,g_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 构建GAN的判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Discriminator(true_data, generated_data, dropout_rate):\n",
    " \n",
    "    # 合并输入数据，包括真实数据true_data和生成器生成的假数据generated_data\n",
    "    sum_data = tf.concat([true_data, generated_data], 0) \n",
    " \n",
    "    # 第一个链接层\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size, h2_size], stddev=0.1), name=\"d_w1\", dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.zeros([h2_size]), name=\"d_b1\", dtype=tf.float32)\n",
    "    h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(sum_data, w1) + b1), dropout_rate)\n",
    " \n",
    "    # 第二个链接层\n",
    "    w2 = tf.Variable(tf.truncated_normal([h2_size, h1_size], stddev=0.1), name=\"d_w2\", dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.zeros([h1_size]), name=\"d_b2\", dtype=tf.float32)\n",
    "    h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1, w2) + b2), dropout_rate)\n",
    " \n",
    "    # 第三个链接层\n",
    "    w3 = tf.Variable(tf.truncated_normal([h1_size, 1], stddev=0.1), name=\"d_w3\", dtype=tf.float32)\n",
    "    b3 = tf.Variable(tf.zeros([1]), name=\"d_b3\", dtype=tf.float32)\n",
    "    h3 = tf.matmul(h2, w3) + b3\n",
    " \n",
    "    #从h3中切出batch_size张图像\n",
    "    slice_image = tf.nn.sigmoid(tf.slice(h3, [0, 0], [batch_size, -1], name=None))\n",
    "    #从h3中切除余下的图像\n",
    "    slice_left_image = tf.nn.sigmoid(tf.slice(h3, [batch_size, 0], [-1, -1], name=None))\n",
    " \n",
    "    #合并参数\n",
    "    d_parameters = [w1, w2, w3, b1, b2, b3]\n",
    " \n",
    "    return slice_image, slice_left_image, d_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 显示结果的函数，结果图片输出到output_image文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowResult(batch_res, filepath, grid_size=(8, 8), grid_pad=5):\n",
    "    \n",
    "    #将batch_res进行值[0, 1]归一化，同时将其reshape成（batch_size, image_height, image_width）\n",
    "    batch_res = 0.5 * batch_res.reshape((batch_res.shape[0], image_height, image_width)) + 0.5\n",
    "   \n",
    "    #重构显示图像格网的参数\n",
    "    re_image_height, re_image_width = batch_res.shape[1], batch_res.shape[2]\n",
    "    grid_height = re_image_height * grid_size[0] + grid_pad * (grid_size[0] - 1)\n",
    "    grid_width = re_image_width  * grid_size[1] + grid_pad * (grid_size[1] - 1)\n",
    "    img_grid = np.zeros((grid_height, grid_width), dtype=np.uint8)\n",
    "    for i, res in enumerate(batch_res):\n",
    "        if i >= grid_size[0] * grid_size[1]:\n",
    "            break\n",
    "        img = (res) * 255.\n",
    "        img = img.astype(np.uint8)\n",
    "        row = (i // grid_size[0]) * (re_image_height + grid_pad)\n",
    "        col = (i % grid_size[1]) * (re_image_width + grid_pad)\n",
    "        img_grid[row:row + re_image_height, col:col + re_image_width] = img\n",
    "    #保存图像\n",
    "    imsave(filepath, img_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StartTrain():\n",
    "\n",
    "    # 加载数据\n",
    "    train_data, train_label = DataLoad(\"./data/MNIST_data\")\n",
    "    size = train_data.shape[0]\n",
    " \n",
    "    # 构建模型---------------------------------------------------------------------\n",
    "    # 定义GAN网络的输入，其中x_data为[batch_size, image_size], z_input为[batch_size, z_size]\n",
    "    x_data = tf.placeholder(tf.float32, [batch_size, image_size], name=\"x_data\") # (batch_size, image_size)\n",
    "    z_input = tf.placeholder(tf.float32, [batch_size, z_size], name=\"z_input\") # (batch_size, z_size)\n",
    "    # 定义dropout率\n",
    "    dropout_rate = tf.placeholder(tf.float32, name=\"dropout_rate\") \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    " \n",
    "    # 利用生成器生成数据x_generated和参数g_params\n",
    "    x_generated, g_params = Generator(z_input)\n",
    "    # 利用判别器判别生成器的结果\n",
    "    y_data, y_generated, d_params = Discriminator(x_data, x_generated, dropout_rate)\n",
    " \n",
    "    # 定义判别器和生成器的loss函数\n",
    "    d_loss = - (tf.log(y_data) + tf.log(1 - y_generated))\n",
    "    g_loss = - tf.log(y_generated)\n",
    " \n",
    "    # 设置学习率为0.0001，用AdamOptimizer进行优化\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001)\n",
    " \n",
    "    # 判别器discriminator 和生成器 generator 对损失函数进行最小化处理\n",
    "    d_trainer = optimizer.minimize(d_loss, var_list=d_params)\n",
    "    g_trainer = optimizer.minimize(g_loss, var_list=g_params)\n",
    "    # 模型构建完毕--------------------------------------------------------------------\n",
    " \n",
    "    # 全局变量初始化\n",
    "    init = tf.global_variables_initializer()\n",
    " \n",
    "    # 启动会话sess\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    " \n",
    "    # 判断是否需要存储\n",
    "    if restore:\n",
    "        #若是，将最近一次的checkpoint点存到outpath下\n",
    "        chkpt_fname = tf.train.latest_checkpoint(output_path)\n",
    "        saver.restore(sess, chkpt_fname)\n",
    "    else:\n",
    "        #若否，判断目录是存在，如果目录存在，则递归的删除目录下的所有内容，并重新建立目录\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)\n",
    "        os.mkdir(output_path)\n",
    " \n",
    "    # 利用随机正态分布产生噪声影像，尺寸为(batch_size, z_size)\n",
    "    z_sample_val = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    " \n",
    "    # 逐个epoch内训练\n",
    "    for i in range(sess.run(global_step), max_epoch):\n",
    "        # 图像每个epoch内可以放(size // batch_size)个size\n",
    "        for j in range(size // batch_size):\n",
    "            if j%20 == 0:\n",
    "                print(\"epoch:%s, iter:%s\" % (i, j))\n",
    "            \n",
    "            # 训练一个batch的数据\n",
    "            batch_end = j * batch_size + batch_size\n",
    "            if batch_end >= size:\n",
    "                batch_end = size - 1\n",
    "            x_value = train_data[ j * batch_size : batch_end ]\n",
    "            # 将数据归一化到[-1, 1]\n",
    "            x_value = x_value / 255.\n",
    "            x_value = 2 * x_value - 1\n",
    "            \n",
    "            # 以正太分布的形式产生随机噪声\n",
    "            z_value = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "            # 每个batch下，输入数据运行GAN，训练判别器\n",
    "            sess.run(d_trainer,\n",
    "                     feed_dict={x_data: x_value, z_input: z_value, dropout_rate: np.sum(0.7).astype(np.float32)})\n",
    "            # 每个batch下，输入数据运行GAN，训练生成器\n",
    "            if j % 1 == 0:\n",
    "                sess.run(g_trainer,\n",
    "                         feed_dict={x_data: x_value,z_input: z_value, dropout_rate: np.sum(0.7).astype(np.float32)})\n",
    "        # 每一个epoch中的所有batch训练完后，利用z_sample测试训练后的生成器\n",
    "        x_gen_val = sess.run(x_generated, feed_dict={z_input: z_sample_val})\n",
    "        # 每一个epoch中的所有batch训练完后，显示生成器的结果，并打印生成结果的值\n",
    "        ShowResult(x_gen_val, os.path.join(output_path, \"sample%s.jpg\" % i))\n",
    "        print(x_gen_val)\n",
    "        # 每一个epoch中，生成随机分布以重置z_random_sample_val\n",
    "        z_random_sample_val = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "        # 每一个epoch中，利用z_random_sample_val生成手写数字图像，并显示结果\n",
    "        x_gen_val = sess.run(x_generated, feed_dict={z_input: z_random_sample_val})\n",
    "        ShowResult(x_gen_val, os.path.join(output_path, \"random_sample%s.jpg\" % i))\n",
    "        # 保存会话\n",
    "        sess.run(tf.assign(global_step, i + 1))\n",
    "        saver.save(sess, os.path.join(output_path, \"model\"), global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iter:0\n",
      "epoch:0, iter:20\n",
      "epoch:0, iter:40\n",
      "epoch:0, iter:60\n",
      "epoch:0, iter:80\n",
      "epoch:0, iter:100\n",
      "epoch:0, iter:120\n",
      "epoch:0, iter:140\n",
      "epoch:0, iter:160\n",
      "epoch:0, iter:180\n",
      "epoch:0, iter:200\n",
      "epoch:0, iter:220\n",
      "[[0.         0.9999915  0.         ... 0.         0.         0.99978554]\n",
      " [0.         0.9999782  0.         ... 0.         0.         0.9873895 ]\n",
      " [0.         0.99989575 0.         ... 0.         0.         0.99940664]\n",
      " ...\n",
      " [0.77444166 0.99998367 0.         ... 0.         0.         0.9997942 ]\n",
      " [0.         0.9999719  0.         ... 0.         0.         0.99995947]\n",
      " [0.         0.99999285 0.         ... 0.         0.         0.99930865]]\n",
      "epoch:1, iter:0\n",
      "epoch:1, iter:20\n",
      "epoch:1, iter:40\n",
      "epoch:1, iter:60\n",
      "epoch:1, iter:80\n",
      "epoch:1, iter:100\n",
      "epoch:1, iter:120\n",
      "epoch:1, iter:140\n",
      "epoch:1, iter:160\n",
      "epoch:1, iter:180\n",
      "epoch:1, iter:200\n",
      "epoch:1, iter:220\n",
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " ...\n",
      " [0.4724562 0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]]\n",
      "epoch:2, iter:0\n",
      "epoch:2, iter:20\n",
      "epoch:2, iter:40\n",
      "epoch:2, iter:60\n",
      "epoch:2, iter:80\n",
      "epoch:2, iter:100\n",
      "epoch:2, iter:120\n",
      "epoch:2, iter:140\n",
      "epoch:2, iter:160\n",
      "epoch:2, iter:180\n",
      "epoch:2, iter:200\n",
      "epoch:2, iter:220\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.17306505 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "epoch:3, iter:0\n",
      "epoch:3, iter:20\n",
      "epoch:3, iter:40\n",
      "epoch:3, iter:60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2f93aad9e2a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mStartTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-1b06a7e571e1>\u001b[0m in \u001b[0;36mStartTrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m# 每个batch下，输入数据运行GAN，训练判别器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             sess.run(d_trainer,\n\u001b[1;32m---> 75\u001b[1;33m                      feed_dict={x_data: x_value, z_input: z_value, dropout_rate: np.sum(0.7).astype(np.float32)})\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[1;31m# 每个batch下，输入数据运行GAN，训练生成器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if train:\n",
    "        StartTrain()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
